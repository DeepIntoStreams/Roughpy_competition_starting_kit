{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64799e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import signatory\n",
    "import numpy as np\n",
    "from scoring_program.evaluation import full_evaluation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d6e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj: object, filepath: str):\n",
    "    \"\"\" Generic function to save an object with different methods. \"\"\"\n",
    "    if filepath.endswith('pkl'):\n",
    "        saver = pickle.dump\n",
    "    elif filepath.endswith('pt'):\n",
    "        saver = torch.save\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    with open(filepath, 'wb') as f:\n",
    "        saver(obj, f)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def load_obj(filepath):\n",
    "    \"\"\" Generic function to load an object. \"\"\"\n",
    "    if filepath.endswith('pkl'):\n",
    "        loader = pickle.load\n",
    "    elif filepath.endswith('pt'):\n",
    "        loader = torch.load\n",
    "    elif filepath.endswith('json'):\n",
    "        import json\n",
    "        loader = json.load\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return loader(f)\n",
    "    \n",
    "    \n",
    "def to_numpy(x):\n",
    "    \"\"\"\n",
    "    Casts torch.Tensor to a numpy ndarray.\n",
    "\n",
    "    The function detaches the tensor from its gradients, then puts it onto the cpu and at last casts it to numpy.\n",
    "    \"\"\"\n",
    "    return x.detach().cpu().numpy()\n",
    "\n",
    "def sample_indices(dataset_size, batch_size):\n",
    "    indices = torch.from_numpy(np.random.choice(\n",
    "        dataset_size, size=batch_size, replace=False)).cuda()\n",
    "    # functions torch.-multinomial and torch.-choice are extremely slow -> back to numpy\n",
    "    return indices.long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cd217a",
   "metadata": {},
   "source": [
    "Define a Generator, which implements the Euler-Maruyama scheme for the log-Heston SDE. The parameters that define the SDE are trainable `torch.Parameter`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33e0641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EulerGenerator(nn.Module):\n",
    "    \"\"\"Given a set of parameters for the log-Heston SDE (as trainable pytorch parameters), produces samples of the SDE\n",
    "    using the Euler-Maruyama scheme from given initial conditions and Brownian increments.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        mu=0.05,\n",
    "        rho1=0.0, rho2=0.0,\n",
    "        kappa1=2.0, theta1=0.04, sigma1=0.2,\n",
    "        kappa2=1.0, theta2=0.1, sigma2=0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mu      = nn.Parameter(torch.tensor(mu))\n",
    "        self.rho1    = nn.Parameter(torch.tensor(rho1))\n",
    "        self.rho2    = nn.Parameter(torch.tensor(rho2))\n",
    "        self.kappa1  = nn.Parameter(torch.tensor(kappa1))\n",
    "        self.theta1  = nn.Parameter(torch.tensor(theta1))\n",
    "        self.sigma1  = nn.Parameter(torch.tensor(sigma1))\n",
    "        self.kappa2  = nn.Parameter(torch.tensor(kappa2))\n",
    "        self.theta2  = nn.Parameter(torch.tensor(theta2))\n",
    "        self.sigma2  = nn.Parameter(torch.tensor(sigma2))\n",
    "        \n",
    "        self.lr_D = 0.0001\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.parameters(), lr=self.lr_D, betas=(0, 0.9))\n",
    "        \n",
    "        self.s = 100.0\n",
    "        self.y1 = 0.4\n",
    "        self.y2 = 0.4\n",
    "\n",
    "    def one_step_forward(self, bm_increment, dt, s, y1, y2):\n",
    "        dw1, dw2, dw3, dw4 = bm_increment[:, 0], bm_increment[:, 1], bm_increment[:, 2], bm_increment[:, 3]\n",
    "\n",
    "        # Ensure non-negative under square root\n",
    "        y1_safe = torch.maximum(torch.zeros_like(y1), y1)\n",
    "        y2_safe = torch.maximum(torch.zeros_like(y2), y2)\n",
    "\n",
    "        # Y1, Y2 Euler steps\n",
    "        y1_new = (y1\n",
    "                  + self.kappa1 * (self.theta1 - y1) * dt\n",
    "                  + self.sigma1 * torch.sqrt(y1_safe) * dw2\n",
    "                  )\n",
    "        y2_new = (y2\n",
    "                  + self.kappa2 * (self.theta2 - y2) * dt\n",
    "                  + self.sigma2 * torch.sqrt(y2_safe) * dw4\n",
    "                  )\n",
    "\n",
    "        # Drift term\n",
    "        dt_term = (self.mu\n",
    "                   - 0.5 * y1 * (self.rho1**2 + 1 - self.rho1)\n",
    "                   - 0.5 * y2 * (self.rho2**2 + 1 - self.rho2))\n",
    "\n",
    "        # S Euler step\n",
    "        s_new = (s\n",
    "                 + dt_term * dt\n",
    "                 + torch.sqrt(y1_safe) * (self.rho1 * dw1 + torch.sqrt(1 - self.rho1) * dw2)\n",
    "                 + torch.sqrt(y2_safe) * (self.rho2 * dw3 + torch.sqrt(1 - self.rho2) * dw4)\n",
    "                 )\n",
    "\n",
    "        return s_new, y1_new, y2_new\n",
    "    \n",
    "    def forward(self,\n",
    "                bm_increments, s_initial_value=None, y1_initial_value=None, y2_initial_value=None):\n",
    "        device = bm_increments.device\n",
    "        N, T, D = bm_increments.shape\n",
    "\n",
    "        if s_initial_value is None:\n",
    "            s_initial_value = torch.tensor(self.s)\n",
    "        if y1_initial_value is None:\n",
    "            y1_initial_value = torch.tensor(self.y1)\n",
    "        if y2_initial_value is None:\n",
    "            y2_initial_value = torch.tensor(self.y2)\n",
    "        \n",
    "        s_path = torch.zeros(N, T+1).to(device)\n",
    "        y1_path = torch.zeros(N, T+1).to(device)\n",
    "        y2_path = torch.zeros(N, T+1).to(device)\n",
    "        s_path[:, 0] = torch.log(s_initial_value)\n",
    "        y1_path[:, 0] = y1_initial_value\n",
    "        y2_path[:, 0] = y2_initial_value\n",
    "        dt = 1/T\n",
    "        for t in range(T):\n",
    "            bm_increment = bm_increments[:, t]\n",
    "            old_s, old_y1, old_y2 = s_path[:, t].clone(), y1_path[:, t].clone(), y2_path[:, t].clone()\n",
    "            log_s, y1, y2 = self.one_step_forward(bm_increment, dt, old_s, old_y1, old_y2)\n",
    "            s_path[:, t+1] = log_s.clone()\n",
    "            y1_path[:, t+1] = y1.clone()\n",
    "            y2_path[:, t+1] = y2.clone()\n",
    "            del old_s, old_y1, old_y2, log_s, y1, y2\n",
    "        s_path = torch.exp(s_path)\n",
    "        return s_path, y1_path, y2_path\n",
    "    \n",
    "    def print_params(self):\n",
    "        print(f\"mu: {self.mu.item()}\")\n",
    "        print(f\"rho1: {self.rho1.item()}\")\n",
    "        print(f\"rho2: {self.rho2.item()}\")\n",
    "        print(f\"kappa1: {self.kappa1.item()}\")\n",
    "        print(f\"theta1: {self.theta1.item()}\")\n",
    "        print(f\"sigma1: {self.sigma1.item()}\")\n",
    "        print(f\"kappa2: {self.kappa2.item()}\")\n",
    "        print(f\"theta2: {self.theta2.item()}\")\n",
    "        print(f\"sigma2: {self.sigma2.item()}\")\n",
    "\n",
    "    def return_params(self):\n",
    "        res_dict = {\"mu\": self.mu.item(),\n",
    "                    \"rho1\": self.rho1.item(),\n",
    "                    \"rho2\": self.rho2.item(),\n",
    "                    \"kappa1\": self.kappa1.item(),\n",
    "                    \"theta1\": self.theta1.item(),\n",
    "                    \"sigma1\": self.sigma1.item(),\n",
    "                    \"kappa2\": self.kappa2.item(),\n",
    "                    \"theta2\": self.theta2.item(),\n",
    "                    \"sigma2\": self.sigma2.item()}\n",
    "        return res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d39b537",
   "metadata": {},
   "source": [
    "Load the training data, which is a single long path. For the calibration, we split it into paths of smaller length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1319960d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data before splitting: torch.Size([1, 20481, 3])\n",
      "training data after splitting: torch.Size([80, 256, 3])\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "x_real = torch.tensor(load_obj(\"long_s_path.pkl\")).to(device)\n",
    "print(f\"training data before splitting: {x_real.shape}\")\n",
    "\n",
    "\n",
    "T = 256\n",
    "num_batches = x_real.shape[1] // T \n",
    "x_real = x_real[:, :(T*num_batches), :]\n",
    "x_real = x_real.reshape((-1, T, 3))\n",
    "print(f\"training data after splitting: {x_real.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ea647",
   "metadata": {},
   "source": [
    "Initialise a Generator with some initial configuration of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e32c9ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'mu': 0.04,\n",
    "        'rho1': 0.5,\n",
    "        'rho2': -0.5,\n",
    "        'kappa1': 3.0,\n",
    "        'theta1': 0.04,\n",
    "        'sigma1': 0.3,\n",
    "        'kappa2': 2.0,\n",
    "        'theta2': 0.1,\n",
    "        'sigma2': 0.5,\n",
    "    }\n",
    "\n",
    "\n",
    "# Initialise Generator\n",
    "calibrator = EulerGenerator(**config)\n",
    "calibrator = calibrator.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697bf3a7",
   "metadata": {},
   "source": [
    "Train the SDE parameters. As a loss, we use the L2 difference between the mean level 2 signature of the set of real paths, and sets of randomly sampled paths from the Generator. Note that this serves purely as an illustration, and we do not claim that this is a particularly good approach (in fact it is not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19a1648a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: loss = 903120.0625\n",
      "Iteration 2: loss = 106517360.0\n",
      "Iteration 3: loss = 103714784.0\n",
      "Iteration 4: loss = 613904.5625\n",
      "Iteration 5: loss = 72676712.0\n",
      "Iteration 6: loss = 18460860.0\n",
      "Iteration 7: loss = 767446.5625\n",
      "Iteration 8: loss = 505198.78125\n",
      "Iteration 9: loss = 10623078.0\n",
      "Iteration 10: loss = 1044736.0625\n",
      "Iteration 11: loss = 6903982.0\n",
      "Iteration 12: loss = 6010684.0\n",
      "Iteration 13: loss = 6675022.5\n",
      "Iteration 14: loss = 192309.421875\n",
      "Iteration 15: loss = 933901.5625\n",
      "Iteration 16: loss = 979219.625\n",
      "Iteration 17: loss = 2973370.75\n",
      "Iteration 18: loss = 675561.9375\n",
      "Iteration 19: loss = 892037.1875\n",
      "Iteration 20: loss = 1941311.375\n"
     ]
    }
   ],
   "source": [
    "bm_dim = 4\n",
    "dt = 1/T\n",
    "\n",
    "losses = []\n",
    "num_iterations = 20\n",
    "for iter in range(num_iterations):\n",
    "    calibrator.optimizer.zero_grad()\n",
    "\n",
    "    x_real_batch = x_real.clone()\n",
    "    s_initial_value = x_real_batch[:, -1, 0].to(device)\n",
    "    y1_initial_value = x_real_batch[:, -1, 1].to(device)\n",
    "    y2_initial_value = x_real_batch[:, -1, 2].to(device)\n",
    "\n",
    "    # Generate set of paths with current training parameters\n",
    "    bm_increments = torch.sqrt(torch.tensor(dt)) * torch.randn([num_batches, T, bm_dim])\n",
    "    x_fake_batch = calibrator.forward(\n",
    "        bm_increments.to(device),\n",
    "        s_initial_value,\n",
    "        y1_initial_value,\n",
    "        y2_initial_value\n",
    "        )\n",
    "    x_fake_batch = torch.stack(x_fake_batch, -1)\n",
    "    \n",
    "    # calculate mean signature of the set of training paths, and the set of samples paths\n",
    "    esig_fake = signatory.signature(x_fake_batch, 2).mean(0)\n",
    "    esig_real = signatory.signature(x_real_batch, 2).mean(0)\n",
    "    \n",
    "    # calculate the L2 norm of the difference\n",
    "    loss = torch.norm(esig_fake-esig_real)\n",
    "    losses.append(loss)\n",
    "\n",
    "\n",
    "    loss.backward()\n",
    "    calibrator.optimizer.step()\n",
    "    print(f\"Iteration {iter+1}: loss = {loss.item()}\")\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44e2bc1",
   "metadata": {},
   "source": [
    "Compare calibrated and real parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97411f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final calibrated parameters:\n",
      "mu: 0.03943103924393654\n",
      "rho1: 0.5001426935195923\n",
      "rho2: -0.4999622404575348\n",
      "kappa1: 3.0000367164611816\n",
      "theta1: 0.0396575927734375\n",
      "sigma1: 0.3000560402870178\n",
      "kappa2: 1.999850869178772\n",
      "theta2: 0.10008596628904343\n",
      "sigma2: 0.5003318190574646\n"
     ]
    }
   ],
   "source": [
    "print(\"Final calibrated parameters:\")\n",
    "calibrator.print_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b005d06a-0f9b-4dcc-b052-5dbb1d559668",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_params = calibrator.return_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdf310f7-66ad-427f-91d7-987dff1636ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Save the learnt parameters into the solution folder\")\n",
    "save_obj(calibrated_params, \"sample_submission_bundle/calibrated_params.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e176dc9b-4647-4bc6-a393-ffc4c03ac630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
